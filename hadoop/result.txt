2019-12-21 11:22:08,929 INFO spark.SparkContext: Running Spark version 2.4.4
2019-12-21 11:22:08,974 INFO spark.SparkContext: Submitted application: Goodreads-tf-idf
2019-12-21 11:22:09,030 INFO spark.SecurityManager: Changing view acls to: root
2019-12-21 11:22:09,030 INFO spark.SecurityManager: Changing modify acls to: root
2019-12-21 11:22:09,031 INFO spark.SecurityManager: Changing view acls groups to: 
2019-12-21 11:22:09,031 INFO spark.SecurityManager: Changing modify acls groups to: 
2019-12-21 11:22:09,031 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2019-12-21 11:22:09,391 INFO util.Utils: Successfully started service 'sparkDriver' on port 37325.
2019-12-21 11:22:09,422 INFO spark.SparkEnv: Registering MapOutputTracker
2019-12-21 11:22:09,445 INFO spark.SparkEnv: Registering BlockManagerMaster
2019-12-21 11:22:09,448 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-12-21 11:22:09,448 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2019-12-21 11:22:09,460 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b7ce7cfc-318d-43cf-8d23-ccc61873874e
2019-12-21 11:22:09,483 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
2019-12-21 11:22:09,502 INFO spark.SparkEnv: Registering OutputCommitCoordinator
2019-12-21 11:22:09,599 INFO util.log: Logging initialized @2927ms
2019-12-21 11:22:09,690 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-12-21 11:22:09,711 INFO server.Server: Started @3041ms
2019-12-21 11:22:09,736 INFO server.AbstractConnector: Started ServerConnector@35dbd6d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-12-21 11:22:09,736 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
2019-12-21 11:22:09,776 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6be87238{/jobs,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,776 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52425da3{/jobs/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,777 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4adf0477{/jobs/job,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,778 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a0edb18{/jobs/job/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,780 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a613cbf{/stages,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,783 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18af6938{/stages/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,785 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4133469d{/stages/stage,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,788 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ddb8836{/stages/stage/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,790 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@468732be{/stages/pool,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,792 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57f21d85{/stages/pool/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,792 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fe7b22f{/storage,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,793 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e3c1bdd{/storage/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,794 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61d1c866{/storage/rdd,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,795 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2688ffec{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,795 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@449c57d8{/environment,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,796 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a805e65{/environment/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,797 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@557d33ab{/executors,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,797 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34ebd177{/executors/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,798 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12ef72a5{/executors/threadDump,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,799 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2aa465eb{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,810 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16f8fb7f{/static,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,812 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61c00a45{/,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,816 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3243f7ed{/api,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,817 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44dbb9b6{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,819 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a876ef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-12-21 11:22:09,825 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-17-55.ap-southeast-1.compute.internal:4040
2019-12-21 11:22:10,011 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://ec2-54-254-241-231.ap-southeast-1.compute.amazonaws.com:7077...
2019-12-21 11:22:10,094 INFO client.TransportClientFactory: Successfully created connection to ec2-54-254-241-231.ap-southeast-1.compute.amazonaws.com/172.31.17.55:7077 after 48 ms (0 ms spent in bootstraps)
2019-12-21 11:22:10,303 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20191221112210-0000
2019-12-21 11:22:10,318 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43097.
2019-12-21 11:22:10,319 INFO netty.NettyBlockTransferService: Server created on ip-172-31-17-55.ap-southeast-1.compute.internal:43097
2019-12-21 11:22:10,321 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-12-21 11:22:10,361 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20191221112210-0000/0 on worker-20191221112205-172.31.19.144-39299 (172.31.19.144:39299) with 2 core(s)
2019-12-21 11:22:10,362 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20191221112210-0000/0 on hostPort 172.31.19.144:39299 with 2 core(s), 1024.0 MB RAM
2019-12-21 11:22:10,372 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20191221112210-0000/1 on worker-20191221112204-172.31.17.55-34479 (172.31.17.55:34479) with 2 core(s)
2019-12-21 11:22:10,372 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20191221112210-0000/1 on hostPort 172.31.17.55:34479 with 2 core(s), 1024.0 MB RAM
2019-12-21 11:22:10,376 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20191221112210-0000/2 on worker-20191221112204-172.31.20.115-37403 (172.31.20.115:37403) with 2 core(s)
2019-12-21 11:22:10,377 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20191221112210-0000/2 on hostPort 172.31.20.115:37403 with 2 core(s), 1024.0 MB RAM
2019-12-21 11:22:10,421 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-17-55.ap-southeast-1.compute.internal, 43097, None)
2019-12-21 11:22:10,425 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-172-31-17-55.ap-southeast-1.compute.internal:43097 with 366.3 MB RAM, BlockManagerId(driver, ip-172-31-17-55.ap-southeast-1.compute.internal, 43097, None)
2019-12-21 11:22:10,428 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-17-55.ap-southeast-1.compute.internal, 43097, None)
2019-12-21 11:22:10,429 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-17-55.ap-southeast-1.compute.internal, 43097, None)
2019-12-21 11:22:10,515 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20191221112210-0000/0 is now RUNNING
2019-12-21 11:22:10,515 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20191221112210-0000/2 is now RUNNING
2019-12-21 11:22:10,543 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20191221112210-0000/1 is now RUNNING
2019-12-21 11:22:10,982 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@85c5183{/metrics/json,null,AVAILABLE,@Spark}
2019-12-21 11:22:11,013 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0

Computing he TF-IDF of the word ' the  ':

Calculating the TF first...
The TF is:
Calculating IDF...
The IDF is:  0.10673860658719833

The TF-IDF is:
2019-12-21 11:23:50,436 ERROR cluster.StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED
2019-12-21 11:23:50,519 ERROR util.Utils: Uncaught exception in thread stop-spark-context
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:283)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:227)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:124)
	at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:653)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2042)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1903)
Caused by: org.apache.spark.SparkException: Could not find AppClient.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:135)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:229)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:523)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	... 9 more
Traceback (most recent call last):
  File "/home/ubuntu/good_test.py", line 93, in <module>
    calc_tfidf(word)
  File "/home/ubuntu/good_test.py", line 87, in calc_tfidf
    df2.show()
  File "/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 380, in show
  File "/home/ubuntu/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/home/ubuntu/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o103.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

